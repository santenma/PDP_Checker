import streamlit as st
import requests
from bs4 import BeautifulSoup
import pandas as pd
from collections import Counter
import re
from urllib.parse import urlparse, quote_plus
import time
import nltk
import matplotlib.pyplot as plt
import seaborn as sns
import plotly.express as px
import plotly.graph_objects as go
import warnings
import json
import random
from datetime import datetime

# Importar wordcloud de forma opcional
try:
    from wordcloud import WordCloud
    WORDCLOUD_AVAILABLE = True
except ImportError:
    WORDCLOUD_AVAILABLE = False

# Suprimir advertencias
warnings.filterwarnings('ignore')

# Configuraci√≥n inicial de la p√°gina
st.set_page_config(
    page_title="An√°lisis Competitivo de Productos",
    page_icon="üéØ",
    layout="wide",
    initial_sidebar_state="expanded"
)

# Descargar recursos de NLTK si es necesario
@st.cache_resource
def download_nltk_data():
    """Descarga datos de NLTK necesarios"""
    try:
        nltk.data.find('tokenizers/punkt')
    except LookupError:
        try:
            nltk.download('punkt', quiet=True)
        except Exception:
            pass

    try:
        nltk.data.find('corpora/stopwords')
    except LookupError:
        try:
            nltk.download('stopwords', quiet=True)
        except Exception:
            pass

download_nltk_data()

def main():
    # CSS personalizado mejorado
    st.markdown("""
    <style>
    .main-header {
        font-size: 2.5rem;
        font-weight: 700;
        background: linear-gradient(90deg, #FF6B6B 0%, #4ECDC4 100%);
        -webkit-background-clip: text;
        -webkit-text-fill-color: transparent;
        text-align: center;
        margin-bottom: 2rem;
    }
    .success-message {
        background-color: #d4edda;
        border: 1px solid #c3e6cb;
        border-radius: 0.5rem;
        padding: 1rem 1.5rem;
        margin-bottom: 1rem;
        color: #155724;
    }
    .warning-message {
        background-color: #fff3cd;
        border: 1px solid #ffeeba;
        border-radius: 0.5rem;
        padding: 1rem 1.5rem;
        margin-bottom: 1rem;
        color: #856404;
    }
    .gap-card {
        background: #f8f9fa;
        border-left: 4px solid #FF6B6B;
        padding: 1rem;
        margin: 0.5rem 0;
        border-radius: 0.25rem;
    }
    .feature-card {
        background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
        color: white;
        padding: 1rem;
        border-radius: 0.5rem;
        margin: 0.5rem 0;
    }
    </style>
    """, unsafe_allow_html=True)
    
    # Header principal
    st.markdown('<h1 class="main-header">üéØ An√°lisis Competitivo de Productos</h1>', unsafe_allow_html=True)
    st.markdown("### Analiza productos de la competencia y encuentra oportunidades de mejora")
    
    # Mensaje de estado de librer√≠as
    if not WORDCLOUD_AVAILABLE:
        st.info("‚ÑπÔ∏è WordCloud no est√° disponible. Las nubes de palabras se mostrar√°n como gr√°ficos de barras.")
    
    # Informaci√≥n de ayuda mejorada
    with st.expander("üìö ¬øC√≥mo usar esta herramienta?"):
        st.markdown("""
        ### üéØ Flujo de Trabajo Recomendado
        
        1. **üîó URL de Referencia**: Ingresa la URL de TU producto
        2. **üîç URLs de Competencia**: A√±ade URLs de productos competidores
        3. **üìä An√°lisis Autom√°tico**: La herramienta extraer√° y comparar√°:
           - T√≠tulos y descripciones
           - Caracter√≠sticas y especificaciones
           - Filtros y categor√≠as
           - Precios y posicionamiento
        4. **üéØ An√°lisis de GAPS**: Identifica qu√© le falta a tu producto
        5. **üí° Insights**: Obt√©n recomendaciones basadas en datos
        
        ### ‚úÖ Sitios Compatibles
        - **Amazon** ‚≠ê Mejor compatibilidad
        - **eBay** ‚≠ê Muy buena compatibilidad
        - **AliExpress** ‚úÖ Generalmente funciona
        - **Tiendas peque√±as** ‚úÖ Menos restricciones
        
        ### üö´ Sitios con Restricciones
        - MediaMarkt, PCComponentes, El Corte Ingl√©s
        - Grandes retailers con protecci√≥n anti-bot
        
        ### üí° Tips Pro
        - Usa el **modo agresivo** para sitios dif√≠ciles
        - Aumenta el **delay** entre requests para evitar bloqueos
        - Analiza productos similares, no id√©nticos
        - Combina con Google Shopping para vista de mercado
        """)
    
    # Sidebar mejorado
    st.sidebar.header("‚öôÔ∏è Configuraci√≥n")
    
    # Opciones de an√°lisis
    st.sidebar.subheader("üìã Tipos de An√°lisis")
    analyze_terms = st.sidebar.checkbox("üî§ T√©rminos clave", value=True)
    analyze_filters = st.sidebar.checkbox("üéõÔ∏è Filtros y categor√≠as", value=True)
    analyze_features = st.sidebar.checkbox("‚≠ê Caracter√≠sticas", value=True)
    analyze_gaps = st.sidebar.checkbox("üéØ An√°lisis de GAPS", value=True)
    analyze_pricing = st.sidebar.checkbox("üí∞ An√°lisis de precios", value=True)
    
    if WORDCLOUD_AVAILABLE:
        show_wordcloud = st.sidebar.checkbox("‚òÅÔ∏è Nube de palabras", value=True)
    else:
        show_wordcloud = False
    
    st.sidebar.markdown("---")
    
    # Configuraci√≥n avanzada
    st.sidebar.subheader("üîß Configuraci√≥n Avanzada")
    
    top_n = st.sidebar.slider("üìä Top N resultados", 5, 50, 20)
    delay = st.sidebar.slider("‚è±Ô∏è Delay entre requests (seg)", 0.5, 5.0, 2.0, 0.5)
    
    st.sidebar.markdown("**üõ°Ô∏è Anti-detecci√≥n:**")
    retry_403 = st.sidebar.checkbox("üîÑ Reintentar bloqueados", value=True)
    aggressive_mode = st.sidebar.checkbox("üöÄ Modo agresivo", value=False)
    rotate_headers = st.sidebar.checkbox("üîÑ Rotar User-Agents", value=False)
    
    if aggressive_mode:
        delay = max(delay, 3.0)
    
    # Pesta√±as principales
    tab1, tab2, tab3 = st.tabs(["üìä An√°lisis de URLs", "üõí Google Shopping", "üìà Comparaci√≥n"])
    
    with tab1:
        st.header("üîó An√°lisis de Productos por URL")
        
        # URL de referencia (nuevo)
        st.subheader("üéØ Producto de Referencia (Tu Producto)")
        reference_url = st.text_input(
            "URL de tu producto (opcional - para an√°lisis de gaps):",
            placeholder="https://tu-tienda.com/tu-producto",
            help="Esta ser√° la referencia para comparar con la competencia"
        )
        
        # URLs de competencia
        st.subheader("üîç Productos de la Competencia")
        
        col1, col2 = st.columns([3, 1])
        
        with col1:
            urls_input = st.text_area(
                "URLs de productos competidores (una por l√≠nea):",
                height=200,
                placeholder="""https://www.amazon.es/dp/B08N5WRWNW
https://www.ebay.es/itm/123456789
https://www.aliexpress.com/item/1005001234567890.html""",
                help="Ingresa las URLs de productos que quieres analizar"
            )
        
        with col2:
            st.markdown("**‚úÖ Compatibles:**")
            st.success("amazon.es/com")
            st.success("ebay.es/com")
            st.success("aliexpress.com")
            st.markdown("**‚ö†Ô∏è Dif√≠ciles:**")
            st.warning("mediamarkt.es")
            st.warning("pccomponentes.com")
        
        # Validaci√≥n de URLs
        if urls_input.strip() or reference_url.strip():
            all_urls = []
            
            if reference_url.strip() and reference_url.startswith(('http://', 'https://')):
                all_urls.append(('reference', reference_url.strip()))
            
            if urls_input.strip():
                comp_urls = [url.strip() for url in urls_input.split('\n') if url.strip()]
                for url in comp_urls:
                    if url.startswith(('http://', 'https://')):
                        all_urls.append(('competitor', url))
            
            if all_urls:
                st.success(f"‚úÖ {len(all_urls)} URLs v√°lidas detectadas")
        
        # Bot√≥n de an√°lisis
        col1, col2, col3 = st.columns([1, 2, 1])
        with col2:
            analyze_button = st.button(
                "üöÄ INICIAR AN√ÅLISIS", 
                type="primary", 
                use_container_width=True,
                disabled=not (urls_input.strip() or reference_url.strip())
            )
        
        if analyze_button:
            analyzer = ProductBenchmarkAnalyzer()
            
            # Progreso
            st.markdown("### üîÑ Procesando URLs...")
            progress_bar = st.progress(0)
            status_text = st.empty()
            
            # M√©tricas
            col1, col2, col3 = st.columns(3)
            with col1:
                success_metric = st.metric("‚úÖ Exitosos", 0)
            with col2:
                failed_metric = st.metric("‚ùå Fallidos", 0)
            with col3:
                total_metric = st.metric("üìä Total", len(all_urls))
            
            reference_data = None
            competitor_data = []
            failed_count = 0
            success_count = 0
            
            # Procesar cada URL
            for i, (url_type, url) in enumerate(all_urls):
                status_text.markdown(f'üîç **Procesando {url_type} {i+1}/{len(all_urls)}**  \n`{url[:70]}...`')
                
                if i > 0:
                    time.sleep(delay * 1.5 if aggressive_mode else delay)
                
                data = analyzer.extract_content_from_url(url, rotate_headers)
                
                if data:
                    if url_type == 'reference':
                        reference_data = data
                    else:
                        competitor_data.append(data)
                    success_count += 1
                    success_metric.metric("‚úÖ Exitosos", success_count)
                else:
                    failed_count += 1
                    failed_metric.metric("‚ùå Fallidos", failed_count)
                    
                    # Retry si est√° habilitado
                    if retry_403:
                        status_text.markdown(f'üîÑ **Reintentando...**')
                        time.sleep(5)
                        retry_data = analyzer.extract_content_from_url(url, True)
                        if retry_data:
                            if url_type == 'reference':
                                reference_data = retry_data
                            else:
                                competitor_data.append(retry_data)
                            success_count += 1
                            failed_count -= 1
                            success_metric.metric("‚úÖ Exitosos", success_count)
                            failed_metric.metric("‚ùå Fallidos", failed_count)
                
                progress_bar.progress((i + 1) / len(all_urls))
            
            status_text.markdown('‚úÖ **An√°lisis completado**')
            
            # Guardar datos en session state
            st.session_state['reference_data'] = reference_data
            st.session_state['competitor_data'] = competitor_data
            st.session_state['all_data'] = [reference_data] if reference_data else [] + competitor_data
            
            if not st.session_state['all_data']:
                st.error("‚ùå No se pudo extraer informaci√≥n de ninguna URL.")
                return
            
            # Mensaje de √©xito
            st.markdown(f"""
            <div class="success-message">
                <strong>üéâ ¬°An√°lisis completado!</strong><br>
                Se procesaron <strong>{success_count}</strong> de <strong>{len(all_urls)}</strong> productos
            </div>
            """, unsafe_allow_html=True)
            
            # Pesta√±as de resultados
            result_tabs = st.tabs([
                "üìä Resumen", 
                "üéØ An√°lisis de GAPS",
                "üî§ T√©rminos", 
                "üéõÔ∏è Filtros", 
                "‚≠ê Caracter√≠sticas",
                "üí∞ Precios",
                "üìà Visualizaciones",
                "üíæ Exportar"
            ])
            
            with result_tabs[0]:  # Resumen
                st.header("üìä Resumen del An√°lisis")
                
                # M√©tricas principales
                col1, col2, col3, col4 = st.columns(4)
                
                all_data = st.session_state['all_data']
                
                with col1:
                    st.metric("üîó Productos Analizados", len(all_data))
                
                with col2:
                    total_features = sum(len(data.get('features', [])) for data in all_data)
                    st.metric("‚≠ê Total Caracter√≠sticas", total_features)
                
                with col3:
                    total_specs = sum(len(data.get('specifications', {})) for data in all_data)
                    st.metric("üîß Total Especificaciones", total_specs)
                
                with col4:
                    products_with_price = sum(1 for data in all_data if data.get('price'))
                    st.metric("üí∞ Con Precio", products_with_price)
                
                # Tabla resumen
                summary_data = []
                for i, data in enumerate(all_data):
                    is_reference = (i == 0 and reference_data)
                    summary_data.append({
                        'Tipo': 'üéØ Referencia' if is_reference else f'üîç Competidor {i}',
                        'Dominio': data.get('domain', 'N/A'),
                        'T√≠tulo': data.get('title', 'Sin t√≠tulo')[:60] + '...',
                        'Precio': data.get('price', 'N/A'),
                        'Caracter√≠sticas': len(data.get('features', [])),
                        'Especificaciones': len(data.get('specifications', {})),
                        'Filtros': len(data.get('filters', []))
                    })
                
                df_summary = pd.DataFrame(summary_data)
                st.dataframe(df_summary, use_container_width=True, hide_index=True)
            
            with result_tabs[1]:  # An√°lisis de GAPS
                st.header("üéØ An√°lisis de GAPS")
                
                if reference_data and competitor_data:
                    gaps = analyzer.analyze_gaps(reference_data, competitor_data)
                    
                    # Caracter√≠sticas √∫nicas de la competencia
                    if gaps['unique_competitor_features']:
                        st.subheader("‚ö° Caracter√≠sticas que tiene la competencia")
                        st.markdown('<div class="warning-message">', unsafe_allow_html=True)
                        st.markdown("**Oportunidades de mejora detectadas:**")
                        for feature in gaps['unique_competitor_features'][:10]:
                            st.markdown(f"‚Ä¢ {feature}")
                        st.markdown('</div>', unsafe_allow_html=True)
                    
                    # Especificaciones faltantes
                    if gaps['missing_specs']:
                        st.subheader("üìã Especificaciones faltantes")
                        cols = st.columns(3)
                        for i, spec in enumerate(gaps['missing_specs'][:12]):
                            cols[i % 3].warning(f"üìå {spec}")
                    
                    # Filtros que usa la competencia
                    if gaps['missing_filters']:
                        st.subheader("üéõÔ∏è Filtros adicionales en competencia")
                        st.info("Considera a√±adir estos filtros a tu tienda:")
                        filter_df = pd.DataFrame(
                            {'Filtro': gaps['missing_filters'][:20]},
                            index=range(1, min(21, len(gaps['missing_filters'])+1))
                        )
                        st.dataframe(filter_df, use_container_width=True)
                    
                    # An√°lisis de precio
                    if gaps['price_difference']:
                        st.subheader("üí∞ An√°lisis de Precio")
                        price_data = gaps['price_difference']
                        
                        col1, col2, col3 = st.columns(3)
                        with col1:
                            st.metric("Tu Precio", f"{price_data['reference']:.2f}‚Ç¨")
                        with col2:
                            st.metric("Promedio Competencia", f"{price_data['competitors_avg']:.2f}‚Ç¨")
                        with col3:
                            diff_color = "üü¢" if price_data['difference'] < 0 else "üî¥"
                            st.metric(
                                "Diferencia",
                                f"{abs(price_data['difference']):.2f}‚Ç¨",
                                f"{diff_color} {abs(price_data['percentage']):.1f}%"
                            )
                        
                        if price_data['percentage'] > 20:
                            st.warning("‚ö†Ô∏è Tu precio es significativamente mayor que la competencia")
                        elif price_data['percentage'] < -20:
                            st.info("üí° Tu precio es muy competitivo, podr√≠as considerar ajustarlo")
                
                else:
                    st.info("üí° A√±ade una URL de referencia y URLs de competencia para ver el an√°lisis de gaps")
            
            # Resto de pesta√±as con an√°lisis tradicionales...
            with result_tabs[2]:  # T√©rminos
                if analyze_terms:
                    st.header("üî§ T√©rminos M√°s Relevantes")
                    terms = analyzer.analyze_terms(all_data)
                    top_terms = terms.most_common(top_n)
                    
                    if top_terms:
                        df_terms = pd.DataFrame(top_terms, columns=['T√©rmino', 'Frecuencia'])
                        
                        fig = px.bar(
                            df_terms, 
                            x='Frecuencia', 
                            y='T√©rmino',
                            orientation='h',
                            color='Frecuencia',
                            color_continuous_scale='viridis',
                            title="T√©rminos clave m√°s frecuentes"
                        )
                        fig.update_layout(height=600, yaxis={'categoryorder':'total ascending'})
                        st.plotly_chart(fig, use_container_width=True)
            
            with result_tabs[7]:  # Exportar
                st.header("üíæ Exportar Resultados")
                
                # Preparar datos para exportaci√≥n
                export_data = []
                
                for i, data in enumerate(all_data):
                    is_reference = (i == 0 and reference_data)
                    export_data.append({
                        'Tipo': 'Referencia' if is_reference else 'Competidor',
                        'URL': data.get('url', ''),
                        'Dominio': data.get('domain', ''),
                        'T√≠tulo': data.get('title', ''),
                        'Descripci√≥n': data.get('description', '')[:500],
                        'Precio': data.get('price', ''),
                        'Caracter√≠sticas': ' | '.join(data.get('features', [])),
                        'Especificaciones': json.dumps(data.get('specifications', {}), ensure_ascii=False),
                        'Filtros': ' | '.join(data.get('filters', [])),
                        'Categor√≠as': ' | '.join(data.get('categories', [])),
                        'Fecha_Extracci√≥n': data.get('extracted_at', '')
                    })
                
                df_export = pd.DataFrame(export_data)
                
                # Bot√≥n de descarga
                csv = df_export.to_csv(index=False, encoding='utf-8')
                
                col1, col2 = st.columns(2)
                
                with col1:
                    st.download_button(
                        label="üì• Descargar An√°lisis Completo (CSV)",
                        data=csv,
                        file_name=f"analisis_competitivo_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv",
                        mime="text/csv",
                        use_container_width=True
                    )
                
                with col2:
                    # Exportar solo gaps si existe
                    if reference_data and competitor_data:
                        gaps = analyzer.analyze_gaps(reference_data, competitor_data)
                        gaps_text = f"""AN√ÅLISIS DE GAPS - {datetime.now().strftime('%Y-%m-%d %H:%M')}
                        
CARACTER√çSTICAS √öNICAS DE COMPETENCIA:
{chr(10).join('‚Ä¢ ' + f for f in gaps['unique_competitor_features'][:20])}

ESPECIFICACIONES FALTANTES:
{chr(10).join('‚Ä¢ ' + s for s in gaps['missing_specs'][:20])}

FILTROS ADICIONALES EN COMPETENCIA:
{chr(10).join('‚Ä¢ ' + f for f in gaps['missing_filters'][:20])}

AN√ÅLISIS DE PRECIO:
{json.dumps(gaps['price_difference'], indent=2) if gaps['price_difference'] else 'No disponible'}
"""
                        st.download_button(
                            label="üì• Descargar An√°lisis de GAPS (TXT)",
                            data=gaps_text,
                            file_name=f"gaps_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt",
                            mime="text/plain",
                            use_container_width=True
                        )
    
    with tab2:  # Google Shopping
        st.header("üõí An√°lisis con Google Shopping")
        st.info("üí° Analiza el mercado completo sin restricciones de sitios web")
        
        search_query = st.text_input(
            "¬øQu√© producto quieres analizar?",
            placeholder="Ejemplo: auriculares bluetooth deportivos",
            help="Describe el producto para buscar en el mercado"
        )
        
        num_results = st.slider("N√∫mero de resultados", 5, 30, 15)
        
        if st.button("üîç Buscar en Google Shopping", type="primary", disabled=not search_query):
            shopping_analyzer = GoogleShoppingAnalyzer()
            
            with st.spinner("Buscando productos..."):
                products, error = shopping_analyzer.search_products_free(search_query, num_results)

            if error:
                st.error(f"No se pudieron obtener resultados: {error}")
            elif products:
                st.success(f"‚úÖ Se encontraron {len(products)} productos")
                
                analysis = shopping_analyzer.analyze_shopping_data(products)
                
                # Sub-pesta√±as para resultados
                shop_tabs = st.tabs(["üìã Productos", "üìä Tiendas", "üí∞ Precios", "üî§ T√©rminos"])
                
                with shop_tabs[0]:
                    st.subheader("Productos Encontrados")
                    for i, product in enumerate(products[:10], 1):
                        col1, col2, col3 = st.columns([3, 1, 1])
                        with col1:
                            st.markdown(f"**{i}. {product.get('title', 'Sin t√≠tulo')[:80]}**")
                        with col2:
                            st.markdown(f"üí∞ **{product.get('price', 'N/A')}**")
                        with col3:
                            st.markdown(f"üè™ {product.get('source', 'N/A')}")
                        st.divider()
                
                with shop_tabs[1]:
                    if analysis['sources']:
                        sources_df = pd.DataFrame(
                            list(analysis['sources'].items()), 
                            columns=['Tienda', 'Productos']
                        ).sort_values('Productos', ascending=False)
                        
                        fig = px.bar(
                            sources_df, 
                            x='Productos', 
                            y='Tienda',
                            orientation='h',
                            title="Distribuci√≥n por Tienda"
                        )
                        st.plotly_chart(fig, use_container_width=True)
                
                with shop_tabs[2]:
                    if analysis.get('price_ranges'):
                        price_info = analysis['price_ranges']
                        col1, col2, col3, col4 = st.columns(4)
                        with col1:
                            st.metric("üíµ M√≠nimo", f"{price_info['min']:.2f}‚Ç¨")
                        with col2:
                            st.metric("üí∏ M√°ximo", f"{price_info['max']:.2f}‚Ç¨")
                        with col3:
                            st.metric("üìä Promedio", f"{price_info['avg']:.2f}‚Ç¨")
                        with col4:
                            st.metric("üî¢ Con Precio", price_info['count'])
                
                with shop_tabs[3]:
                    if analysis['common_terms']:
                        terms_data = analysis['common_terms'].most_common(25)
                        terms_df = pd.DataFrame(terms_data, columns=['T√©rmino', 'Frecuencia'])
                        
                        fig = px.bar(
                            terms_df.head(15),
                            x='Frecuencia',
                            y='T√©rmino',
                            orientation='h',
                            title="T√©rminos m√°s frecuentes",
                            color='Frecuencia',
                            color_continuous_scale='plasma'
                        )
                        fig.update_layout(height=500, yaxis={'categoryorder':'total ascending'})
                        st.plotly_chart(fig, use_container_width=True)
            else:
                st.warning("No se encontraron productos. Intenta con otro t√©rmino.")
    
    with tab3:  # Comparaci√≥n
        st.header("üìà Comparaci√≥n Visual")
        
        if 'all_data' in st.session_state and st.session_state['all_data']:
            all_data = st.session_state['all_data']
            
            # Crear comparaci√≥n visual
            st.subheader("üîç Matriz de Comparaci√≥n")
            
            comparison_matrix = []
            for data in all_data:
                comparison_matrix.append({
                    'Producto': data.get('title', 'Sin t√≠tulo')[:50],
                    'Precio': 1 if data.get('price') else 0,
                    'Descripci√≥n': len(data.get('description', '')) > 100,
                    'Caracter√≠sticas': len(data.get('features', [])),
                    'Especificaciones': len(data.get('specifications', {})),
                    'Im√°genes': len(data.get('images', [])),
                    'Categor√≠as': len(data.get('categories', []))
                })
            
            df_comparison = pd.DataFrame(comparison_matrix)
            
            # Heatmap de caracter√≠sticas
            fig = px.imshow(
                df_comparison.set_index('Producto').T,
                labels=dict(x="Productos", y="Atributos", color="Valor"),
                aspect="auto",
                color_continuous_scale="RdYlGn",
                title="Mapa de Calor - Completitud de Informaci√≥n"
            )
            # Ajustar el √°ngulo de las etiquetas del eje x
            fig.update_xaxes(tickangle=45)
            st.plotly_chart(fig, use_container_width=True)
            
            # Gr√°fico de radar para comparaci√≥n
            if len(all_data) <= 5:  # Solo mostrar radar si hay 5 o menos productos
                st.subheader("üéØ Comparaci√≥n Radar")
                
                categories = ['Caracter√≠sticas', 'Especificaciones', 'Im√°genes', 'Categor√≠as', 'Filtros']
                
                fig = go.Figure()
                
                for i, data in enumerate(all_data):
                    values = [
                        len(data.get('features', [])),
                        len(data.get('specifications', {})),
                        len(data.get('images', [])),
                        len(data.get('categories', [])),
                        len(data.get('filters', []))
                    ]
                    
                    fig.add_trace(go.Scatterpolar(
                        r=values,
                        theta=categories,
                        fill='toself',
                        name=f"Producto {i+1}"
                    ))
                
                fig.update_layout(
                    polar=dict(
                        radialaxis=dict(
                            visible=True,
                            range=[0, max([
                                max(len(d.get('features', [])), 
                                    len(d.get('specifications', {})),
                                    len(d.get('images', [])),
                                    len(d.get('categories', [])),
                                    len(d.get('filters', [])))
                                for d in all_data
                            ])]
                        )
                    ),
                    showlegend=True,
                    title="Comparaci√≥n de Completitud por Producto"
                )
                
                st.plotly_chart(fig, use_container_width=True)
            
            # Insights autom√°ticos
            st.subheader("üí° Insights Autom√°ticos")
            
            insights = []
            
            # An√°lisis de completitud
            completeness_scores = []
            for data in all_data:
                score = 0
                score += 1 if data.get('title') else 0
                score += 1 if data.get('description') else 0
                score += 1 if data.get('price') else 0
                score += min(len(data.get('features', [])) / 5, 1)
                score += min(len(data.get('specifications', {})) / 5, 1)
                score += min(len(data.get('images', [])) / 3, 1)
                completeness_scores.append(score / 6 * 100)
            
            best_product_idx = completeness_scores.index(max(completeness_scores))
            worst_product_idx = completeness_scores.index(min(completeness_scores))
            
            insights.append(f"üìä **Producto m√°s completo**: Producto {best_product_idx + 1} ({completeness_scores[best_product_idx]:.1f}% completitud)")
            insights.append(f"‚ö†Ô∏è **Producto menos completo**: Producto {worst_product_idx + 1} ({completeness_scores[worst_product_idx]:.1f}% completitud)")
            
            # An√°lisis de precios
            prices = []
            for data in all_data:
                if data.get('price'):
                    price_match = re.search(r'[\d,]+\.?\d*', data.get('price', '').replace(',', ''))
                    if price_match:
                        try:
                            prices.append(float(price_match.group()))
                        except:
                            pass
            
            if prices:
                avg_price = sum(prices) / len(prices)
                insights.append(f"üí∞ **Precio promedio**: {avg_price:.2f}‚Ç¨")
                insights.append(f"üíµ **Rango de precios**: {min(prices):.2f}‚Ç¨ - {max(prices):.2f}‚Ç¨")
            
            # Mostrar insights
            for insight in insights:
                st.info(insight)
            
            # Recomendaciones
            st.subheader("üéØ Recomendaciones")
            
            recommendations = []
            
            if 'reference_data' in st.session_state and st.session_state['reference_data']:
                ref_data = st.session_state['reference_data']
                ref_score = completeness_scores[0]
                
                if ref_score < 70:
                    recommendations.append("üî¥ **Urgente**: Tu producto tiene poca informaci√≥n. A√±ade m√°s descripciones y caracter√≠sticas.")
                elif ref_score < 85:
                    recommendations.append("üü° **Importante**: Tu producto est√° bien pero puede mejorar. Considera a√±adir m√°s especificaciones t√©cnicas.")
                else:
                    recommendations.append("üü¢ **Excelente**: Tu producto tiene informaci√≥n muy completa.")
                
                if not ref_data.get('price'):
                    recommendations.append("üí∞ **Precio**: Considera mostrar el precio claramente en la p√°gina del producto.")
                
                if len(ref_data.get('images', [])) < 3:
                    recommendations.append("üì∏ **Im√°genes**: A√±ade m√°s im√°genes del producto (m√≠nimo 3-5).")
            
            if not recommendations:
                recommendations.append("üí° A√±ade una URL de referencia para obtener recomendaciones personalizadas.")
            
            for rec in recommendations:
                st.markdown(rec)
        
        else:
            st.info("üëÜ Primero realiza un an√°lisis en la pesta√±a 'An√°lisis de URLs' para ver comparaciones.")
    
    # Footer con informaci√≥n adicional
    st.markdown("---")
    st.markdown("""
    <div style='text-align: center; color: #666; padding: 2rem;'>
        <p>üéØ <strong>An√°lisis Competitivo de Productos</strong> v2.0</p>
        <p>Desarrollado para ayudarte a mejorar tu estrategia de producto</p>
        <p>üí° Tip: Analiza regularmente a tu competencia para mantenerte actualizado</p>
    </div>
    """, unsafe_allow_html=True)

class GoogleShoppingAnalyzer:
    """Analizador mejorado de Google Shopping con m√∫ltiples m√©todos de extracci√≥n"""
    
    def __init__(self):
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,*/*;q=0.8',
            'Accept-Language': 'es-ES,es;q=0.9,en;q=0.8',
            'Accept-Encoding': 'gzip, deflate, br',
            'DNT': '1',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
            'Sec-Fetch-Dest': 'document',
            'Sec-Fetch-Mode': 'navigate',
            'Sec-Fetch-Site': 'none',
            'Cache-Control': 'max-age=0'
        }
    
    def search_products_free(self, query, num_results=20, country='es'):
        """
        M√©todo mejorado para buscar productos en Google Shopping
        """
        products = []
        
        # M√©todo 1: Google Shopping directo
        products = self._search_google_shopping(query, num_results, country)
        
        # M√©todo 2: Si falla, intentar b√∫squeda regular de Google con filtro de shopping
        if len(products) < 5:
            products.extend(self._search_google_regular(query, num_results, country))
        
        # M√©todo 3: B√∫squeda alternativa con diferentes par√°metros
        if len(products) < 5:
            products.extend(self._search_alternative(query, num_results, country))
        
        # Eliminar duplicados manteniendo orden
        seen = set()
        unique_products = []
        for product in products:
            # Crear una key √∫nica basada en t√≠tulo y precio
            key = f"{product.get('title', '')[:50]}_{product.get('price', '')}"
            if key not in seen:
                seen.add(key)
                unique_products.append(product)
        
        return unique_products[:num_results]
    
    def _search_google_shopping(self, query, num_results, country='es'):
        """B√∫squeda en Google Shopping"""
        try:
            # URLs para diferentes regiones
            base_urls = {
                'es': 'https://www.google.es/search',
                'com': 'https://www.google.com/search',
                'mx': 'https://www.google.com.mx/search'
            }
            
            base_url = base_urls.get(country, base_urls['es'])
            
            # Par√°metros optimizados para Google Shopping
            params = {
                'q': query,
                'tbm': 'shop',  # Modo shopping
                'hl': 'es',     # Idioma
                'gl': country,  # Pa√≠s
                'num': num_results,
                'safe': 'off',
                'tbs': 'vw:g'   # Vista de grid
            }
            
            url = base_url + '?' + '&'.join([f"{k}={quote_plus(str(v))}" for k, v in params.items()])
            
            response = requests.get(url, headers=self.headers, timeout=15)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.content, 'html.parser')
            products = []
            
            # Selectores actualizados para Google Shopping 2024-2025
            product_selectors = [
                # Selectores principales de Google Shopping
                'div[class*="sh-dgr__content"]',
                'div[class*="sh-pr__product-results"] div[class*="sh-dlr__list-result"]',
                'div[class*="KZmu8e"]',
                'div[class*="i0X6df"]',
                'div[data-docid]',
                'div[class*="u30d4"]',
                'div[class*="Rn1jbe"]',
                # Selectores de cards de productos
                'div[jsname="N9Xkfe"]',
                'div[data-merchant-id]',
                'div[class*="xcR77"]'
            ]
            
            for selector in product_selectors:
                elements = soup.select(selector)
                if elements:
                    for element in elements[:num_results]:
                        product = self._extract_product_from_element(element)
                        if product and product.get('title'):
                            products.append(product)
                    
                    if len(products) >= 5:  # Si encontramos suficientes productos, salir
                        break
            
            # Si no encontramos productos con selectores espec√≠ficos, buscar de forma m√°s general
            if len(products) < 5:
                products.extend(self._extract_products_generic(soup, num_results))
            
            return products
            
        except Exception as e:
            st.warning(f"‚ö†Ô∏è Error en b√∫squeda de Google Shopping: {str(e)}")
            return []
    
    def _search_google_regular(self, query, num_results, country='es'):
        """B√∫squeda en Google regular con t√©rminos de shopping"""
        try:
            # Modificar query para incluir t√©rminos de shopping
            shopping_query = f"{query} comprar precio tienda online"
            
            base_url = f"https://www.google.{country}/search"
            params = {
                'q': shopping_query,
                'num': num_results,
                'hl': 'es',
                'gl': country
            }
            
            url = base_url + '?' + '&'.join([f"{k}={quote_plus(str(v))}" for k, v in params.items()])
            
            response = requests.get(url, headers=self.headers, timeout=15)
            soup = BeautifulSoup(response.content, 'html.parser')
            
            products = []
            
            # Buscar resultados que parezcan productos
            result_divs = soup.select('div.g, div[class*="result"], div[data-hveid]')
            
            for div in result_divs[:num_results]:
                # Extraer informaci√≥n que parezca de producto
                title_elem = div.select_one('h3, a')
                if title_elem:
                    title = title_elem.get_text().strip()
                    
                    # Buscar precio en el texto
                    price = self._extract_price_from_text(div.get_text())
                    
                    # Extraer URL
                    link_elem = div.select_one('a[href]')
                    link = link_elem.get('href', '') if link_elem else ''
                    
                    # Identificar tienda
                    source = self._extract_source_from_url(link)
                    
                    if title and (price or 'precio' in title.lower() or '‚Ç¨' in div.get_text()):
                        products.append({
                            'title': title,
                            'price': price or 'Consultar precio',
                            'source': source,
                            'link': link,
                            'description': title,
                            'method': 'Google Search'
                        })
            
            return products
            
        except Exception as e:
            st.warning(f"‚ö†Ô∏è Error en b√∫squeda regular: {str(e)}")
            return []
    
    def _search_alternative(self, query, num_results, country='es'):
        """M√©todo alternativo usando b√∫squeda de im√°genes de productos"""
        try:
            # B√∫squeda de im√°genes de Google Shopping
            base_url = f"https://www.google.{country}/search"
            params = {
                'q': query,
                'tbm': 'isch',  # Im√°genes
                'tbs': 'cat:530',  # Categor√≠a de productos
                'hl': 'es',
                'gl': country
            }
            
            url = base_url + '?' + '&'.join([f"{k}={quote_plus(str(v))}" for k, v in params.items()])
            
            response = requests.get(url, headers=self.headers, timeout=15)
            soup = BeautifulSoup(response.content, 'html.parser')
            
            products = []
            
            # Buscar metadatos de productos en las im√°genes
            scripts = soup.find_all('script')
            for script in scripts:
                text = script.string
                if text and 'data:' in text:
                    # Buscar patrones de productos en JSON
                    products.extend(self._extract_products_from_json(text))
            
            return products[:num_results]
            
        except Exception as e:
            st.warning(f"‚ö†Ô∏è Error en b√∫squeda alternativa: {str(e)}")
            return []
    
    def _extract_product_from_element(self, element):
        """Extrae informaci√≥n del producto de un elemento HTML"""
        try:
            product = {}
            
            # T√≠tulo - m√∫ltiples selectores
            title_selectors = [
                'h3', 'h4', 'a[class*="DdKZJb"]', 
                'div[class*="rgHvZc"]', 'div[class*="EI11Pd"]',
                'span[class*="Q8U8"]', 'div[class*="Xjkr3b"]',
                'div[class*="ArOc1c"]', 'a[aria-label]'
            ]
            
            for selector in title_selectors:
                title_elem = element.select_one(selector)
                if title_elem:
                    title_text = title_elem.get_text().strip()
                    if not title_text and title_elem.get('aria-label'):
                        title_text = title_elem.get('aria-label')
                    if title_text and len(title_text) > 10:
                        product['title'] = title_text
                        break
            
            # Precio - m√∫ltiples selectores
            price_selectors = [
                'span[class*="a8Pemb"]', 'span[class*="OFFNJ"]',
                'div[class*="OFFNJ"]', 'span[class*="Nr22bf"]',
                'span[class*="HRLxBb"]', 'div[class*="HRLxBb"]',
                'span:contains("‚Ç¨")', 'span:contains("EUR")',
                '*[aria-label*="precio"]', '*[aria-label*="‚Ç¨"]'
            ]
            
            for selector in price_selectors:
                if ':contains' in selector:
                    # Buscar elementos que contengan el s√≠mbolo de euro
                    price_elems = element.find_all(text=re.compile('[‚Ç¨$]|EUR'))
                    for price_text in price_elems:
                        if price_text and '‚Ç¨' in str(price_text):
                            product['price'] = str(price_text).strip()
                            break
                else:
                    price_elem = element.select_one(selector)
                    if price_elem:
                        price_text = price_elem.get_text().strip()
                        if not price_text and price_elem.get('aria-label'):
                            price_text = price_elem.get('aria-label')
                        if price_text and ('‚Ç¨' in price_text or 'EUR' in price_text or re.search(r'\d', price_text)):
                            product['price'] = price_text
                            break
            
            # Tienda/Vendedor
            source_selectors = [
                'span[class*="aULzUe"]', 'span[class*="IuHnof"]',
                'div[class*="IuHnof"]', 'span[class*="vjtvZe"]',
                'div[class*="dD8iuc"]', 'span[class*="zPEcBd"]',
                'cite', 'span.VuuXrf'
            ]
            
            for selector in source_selectors:
                source_elem = element.select_one(selector)
                if source_elem:
                    source_text = source_elem.get_text().strip()
                    if source_text and len(source_text) > 2:
                        product['source'] = source_text
                        break
            
            # Link
            link_elem = element.select_one('a[href]')
            if link_elem:
                href = link_elem.get('href', '')
                if href.startswith('/url?'):
                    # Extraer URL real de Google redirect
                    import urllib.parse
                    parsed = urllib.parse.parse_qs(urllib.parse.urlparse(href).query)
                    if 'q' in parsed:
                        product['link'] = parsed['q'][0]
                    elif 'url' in parsed:
                        product['link'] = parsed['url'][0]
                    else:
                        product['link'] = href
                elif href.startswith('http'):
                    product['link'] = href
                else:
                    product['link'] = 'https://www.google.com' + href
            
            # Descripci√≥n adicional
            desc_elem = element.select_one('div[class*="VwiC3b"], span[class*="aCOpRe"]')
            if desc_elem:
                product['description'] = desc_elem.get_text().strip()
            elif product.get('title'):
                product['description'] = product['title']
            
            # Validar que tengamos al menos t√≠tulo
            if product.get('title'):
                product['method'] = 'Google Shopping'
                return product
                
        except Exception as e:
            pass
        
        return None
    
    def _extract_products_generic(self, soup, num_results):
        """Extracci√≥n gen√©rica cuando los selectores espec√≠ficos fallan"""
        products = []
        
        try:
            # Buscar cualquier div que parezca contener un producto
            all_divs = soup.find_all('div', limit=200)
            
            for div in all_divs:
                text = div.get_text()
                
                # Heur√≠stica: si contiene precio, probablemente es un producto
                if '‚Ç¨' in text or 'EUR' in text or re.search(r'\d+[,\.]\d{2}', text):
                    # Buscar t√≠tulo (primer texto largo)
                    lines = [line.strip() for line in text.split('\n') if line.strip()]
                    
                    title = None
                    price = None
                    source = None
                    
                    for line in lines:
                        if not title and len(line) > 20 and len(line) < 200:
                            title = line
                        if not price and ('‚Ç¨' in line or 'EUR' in line):
                            price = line
                        if not source and len(line) < 50 and ('.' in line or 'tienda' in line.lower()):
                            source = line
                    
                    if title:
                        products.append({
                            'title': title,
                            'price': price or 'Ver precio',
                            'source': source or 'Tienda online',
                            'link': '#',
                            'description': title,
                            'method': 'Generic extraction'
                        })
                        
                        if len(products) >= num_results:
                            break
            
        except Exception as e:
            pass
        
        return products
    
    def _extract_price_from_text(self, text):
        """Extrae precio de un texto"""
        # Patrones de precio comunes
        price_patterns = [
            r'(\d+[,\.]\d{2})\s*‚Ç¨',
            r'‚Ç¨\s*(\d+[,\.]\d{2})',
            r'EUR\s*(\d+[,\.]\d{2})',
            r'(\d+)\s*‚Ç¨',
            r'‚Ç¨\s*(\d+)',
            r'(\d+[,\.]\d+)\s*euros?'
        ]
        
        for pattern in price_patterns:
            match = re.search(pattern, text, re.IGNORECASE)
            if match:
                return match.group(0)
        
        return None
    
    def _extract_source_from_url(self, url):
        """Extrae el nombre de la tienda desde la URL"""
        if not url:
            return 'Tienda online'
        
        try:
            from urllib.parse import urlparse
            domain = urlparse(url).netloc
            
            # Limpiar subdominios comunes
            domain = domain.replace('www.', '').replace('.com', '').replace('.es', '')
            
            # Capitalizar primera letra
            return domain.split('.')[0].capitalize()
        except:
            return 'Tienda online'
    
    def _extract_products_from_json(self, text):
        """Intenta extraer productos de datos JSON embebidos"""
        products = []
        
        try:
            # Buscar patrones JSON de productos
            import json
            
            # Buscar arrays JSON
            json_pattern = r'\[.*?"title".*?\]'
            matches = re.findall(json_pattern, text, re.DOTALL)
            
            for match in matches:
                try:
                    data = json.loads(match)
                    if isinstance(data, list):
                        for item in data:
                            if isinstance(item, dict) and 'title' in item:
                                products.append({
                                    'title': item.get('title', ''),
                                    'price': item.get('price', 'Ver precio'),
                                    'source': item.get('merchant', 'Tienda online'),
                                    'link': item.get('link', '#'),
                                    'description': item.get('description', item.get('title', '')),
                                    'method': 'JSON extraction'
                                })
                except:
                    pass
        except:
            pass
        
        return products
    
    def analyze_shopping_data(self, products):
        """Analiza los datos obtenidos de Google Shopping"""
        if not products:
            return {
                'total_products': 0,
                'sources': {},
                'price_ranges': None,
                'common_terms': Counter()
            }
        
        analysis = {
            'total_products': len(products),
            'sources': {},
            'price_ranges': None,
            'common_terms': Counter()
        }
        
        # An√°lisis por fuente
        for product in products:
            source = product.get('source', 'Desconocido').strip()
            if source:
                analysis['sources'][source] = analysis['sources'].get(source, 0) + 1
        
        # An√°lisis de precios mejorado
        prices = []
        for product in products:
            price_text = product.get('price', '')
            if price_text and price_text != 'Ver precio' and price_text != 'Consultar precio':
                # Mejorar extracci√≥n de n√∫meros
                price_numbers = re.findall(r'[\d,]+\.?\d*', price_text.replace(',', '.'))
                for price_str in price_numbers:
                    try:
                        price = float(price_str.replace(',', '.'))
                        if 0.01 < price < 100000:  # Rango razonable
                            prices.append(price)
                            break
                    except:
                        continue
        
        if prices:
            analysis['price_ranges'] = {
                'min': min(prices),
                'max': max(prices),
                'avg': sum(prices) / len(prices),
                'median': sorted(prices)[len(prices)//2],
                'count': len(prices)
            }
        
        # An√°lisis de t√©rminos mejorado
        all_text = ''
        for product in products:
            all_text += f" {product.get('title', '')} {product.get('description', '')} "
        
        # Tokenizaci√≥n mejorada
        words = re.findall(r'\b[a-z√°√©√≠√≥√∫√±√ºA-Z√Å√â√ç√ì√ö√ë√ú]{3,}\b', all_text)
        
        # Stopwords expandidas
        stopwords = {
            'para', 'con', 'por', 'del', 'las', 'los', 'una', 'uno', 'the', 'and', 
            'for', 'with', 'desde', 'hasta', 'm√°s', 'muy', 'todo', 'todos', 'nuevo',
            'nueva', 'comprar', 'precio', 'oferta', 'env√≠o', 'gratis', 'tienda',
            'online', 'mejor', 'bueno', 'excelente', 'calidad', 'producto', 'venta'
        }
        
        # Filtrar y contar
        filtered_words = [word.lower() for word in words if word.lower() not in stopwords and len(word) > 3]
        analysis['common_terms'] = Counter(filtered_words)
        
        return analysis
        
class ProductBenchmarkAnalyzer:
    def __init__(self):
        """Inicializa el analizador con stopwords mejoradas"""
        try:
            # Stopwords b√°sicas en espa√±ol e ingl√©s
            spanish_stopwords = set([
                'el', 'la', 'de', 'que', 'y', 'a', 'en', 'un', 'es', 'se', 'no', 'te', 'lo', 
                'le', 'da', 'su', 'por', 'son', 'con', 'para', 'al', 'del', 'las', 'una', 
                'su', 'me', 'si', 'tu', 'm√°s', 'muy', 'pero', 'como', 'son', 'los', 'este',
                'esta', 'esto', 'ese', 'esa', 'esos', 'esas', 'tiene', 'ser', 'hacer',
                'estar', 'todo', 'todos', 'toda', 'todas', 'cuando', 'donde', 'como',
                'porque', 'aunque', 'desde', 'hasta', 'entre', 'sobre', 'bajo', 'sin'
            ])
            
            english_stopwords = set([
                'the', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for', 'of', 'with', 
                'by', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 
                'had', 'do', 'does', 'did', 'will', 'would', 'could', 'should', 'may', 
                'might', 'must', 'can', 'this', 'that', 'these', 'those', 'all', 'any', 
                'some', 'each', 'every', 'both', 'either', 'neither', 'one', 'two', 'three'
            ])
            
            # Palabras relacionadas con e-commerce que NO queremos analizar
            ecommerce_stopwords = set([
                'a√±adir', 'carrito', 'comprar', 'compra', 'pedido', 'envio', 'env√≠o', 
                'entrega', 'prevista', 'generado', 'stock', 'disponible', 'agotado',
                'precio', 'oferta', 'descuento', 'rebaja', 'promocion', 'promoci√≥n',
                'gratis', 'gratuito', 'iva', 'incluido', 'excluido', 'gastos',
                'valoracion', 'valoraci√≥n', 'opinion', 'opini√≥n', 'comentario',
                'puntuacion', 'puntuaci√≥n', 'estrella', 'estrellas', 'valorar',
                'recomendar', 'recomiendo', 'cliente', 'clientes', 'usuario', 'usuarios',
                'cada', 'solo', 's√≥lo', 'solamente', '√∫nicamente', 'tambi√©n', 'adem√°s',
                'producto', 'productos', 'articulo', 'art√≠culo', 'item', 'items',
                'marca', 'modelo', 'referencia', 'codigo', 'c√≥digo', 'sku',
                'categoria', 'categor√≠a', 'seccion', 'secci√≥n', 'departamento',
                'buscar', 'busqueda', 'b√∫squeda', 'filtrar', 'filtro', 'filtros',
                'ordenar', 'clasificar', 'mostrar', 'ver', 'todos', 'todas',
                'inicio', 'home', 'tienda', 'shop', 'store', 'online',
                'web', 'website', 'pagina', 'p√°gina', 'sitio', 'portal',
                'cookies', 'politica', 'pol√≠tica', 'privacidad', 'terminos', 't√©rminos',
                'condiciones', 'legal', 'aviso', 'contacto', 'ayuda', 'soporte'
            ])
            
            try:
                nltk_spanish = set(nltk.corpus.stopwords.words('spanish'))
                nltk_english = set(nltk.corpus.stopwords.words('english'))
                self.stop_words = spanish_stopwords | english_stopwords | ecommerce_stopwords | nltk_spanish | nltk_english
            except:
                self.stop_words = spanish_stopwords | english_stopwords | ecommerce_stopwords
                
        except:
            # Fallback m√≠nimo
            self.stop_words = set(['el', 'la', 'de', 'que', 'y', 'a', 'en', 'the', 'and', 'or', 'a√±adir', 'carrito', 'entrega', 'envio'])
        
        self.results = []
        self.headers_options = [
            {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
                'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,*/*;q=0.8',
                'Accept-Language': 'es-ES,es;q=0.9,en;q=0.8',
                'Accept-Encoding': 'gzip, deflate, br',
                'DNT': '1',
                'Connection': 'keep-alive',
                'Upgrade-Insecure-Requests': '1'
            },
            {
                'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.0 Safari/605.1.15',
                'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
                'Accept-Language': 'es-es',
                'Accept-Encoding': 'gzip, deflate, br',
                'Connection': 'keep-alive'
            },
            {
                'User-Agent': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
                'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
                'Accept-Language': 'es,en-US;q=0.7,en;q=0.3',
                'Accept-Encoding': 'gzip, deflate',
                'Connection': 'keep-alive'
            }
        ]
        
    def extract_content_from_url(self, url, rotate_headers=False):
        """Extrae contenido relevante de una URL de producto"""
        try:
            # Seleccionar headers
            if rotate_headers:
                headers = random.choice(self.headers_options)
            else:
                headers = self.headers_options[0]
            
            # Usar session para mantener cookies
            session = requests.Session()
            session.headers.update(headers)
            
            response = session.get(url, timeout=20, allow_redirects=True)
            
            # Si obtenemos 403, intentamos estrategias adicionales
            if response.status_code == 403:
                # Estrategia 1: Headers m√≠nimos
                minimal_headers = {
                    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:109.0) Gecko/20100101 Firefox/121.0'
                }
                session.headers.clear()
                session.headers.update(minimal_headers)
                time.sleep(3)
                response = session.get(url, timeout=20, allow_redirects=True)
                
                # Estrategia 2: Si sigue fallando, probar con otro user-agent
                if response.status_code == 403:
                    session.headers.update({
                        'User-Agent': 'Mozilla/5.0 (iPhone; CPU iPhone OS 17_0 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.0 Mobile/15E148 Safari/604.1'
                    })
                    time.sleep(5)
                    response = session.get(url, timeout=20, allow_redirects=True)
            
            response.raise_for_status()
            
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # Extraer informaci√≥n del producto
            product_data = {
                'url': url,
                'domain': urlparse(url).netloc,
                'title': self._extract_title(soup),
                'description': self._extract_description(soup),
                'features': self._extract_features(soup),
                'specifications': self._extract_specifications(soup),
                'price': self._extract_price(soup),
                'filters': self._extract_filters(soup),
                'categories': self._extract_categories(soup),
                'images': self._extract_images(soup),
                'extracted_at': datetime.now().isoformat()
            }
            
            return product_data
            
        except requests.exceptions.HTTPError as e:
            if e.response.status_code == 403:
                domain = urlparse(url).netloc
                st.warning(f"üö´ Acceso denegado a {domain}")
                self._suggest_alternatives(domain)
            else:
                st.warning(f"‚ö†Ô∏è Error HTTP {e.response.status_code} con {url[:50]}...")
            return None
        except requests.exceptions.RequestException as e:
            st.warning(f"‚ö†Ô∏è Error de conexi√≥n con {url[:50]}...: {str(e)}")
            return None
        except Exception as e:
            st.warning(f"‚ö†Ô∏è Error procesando {url[:50]}...: {str(e)}")
            return None
    
    def _suggest_alternatives(self, domain):
        """Sugiere alternativas para sitios bloqueados"""
        alternatives = {
            'mediamarkt': "üí° **Alternativa para MediaMarkt:** Busca el mismo producto en Amazon o eBay",
            'pccomponentes': "üí° **Alternativa para PCComponentes:** Prueba con Amazon o tiendas especializadas",
            'elcorteingles': "üí° **Alternativa para El Corte Ingl√©s:** Busca en Amazon o tiendas del fabricante"
        }
        
        for site, message in alternatives.items():
            if site in domain.lower():
                st.info(message)
                break
    
    def _extract_title(self, soup):
        """Extrae el t√≠tulo del producto"""
        selectors = [
            'h1[class*="title"]',
            'h1[class*="product"]',
            '[data-testid*="title"]',
            '[class*="product-title"]',
            '[class*="product-name"]',
            '[id*="title"]',
            'h1',
            'title'
        ]
        
        for selector in selectors:
            elements = soup.select(selector)
            for element in elements:
                text = element.get_text().strip()
                if text and len(text) > 5 and len(text) < 300:
                    return text
        return ""
    
    def _extract_description(self, soup):
        """Extrae la descripci√≥n del producto enfoc√°ndose en contenido relevante"""
        description = ""
        
        # Selectores espec√≠ficos para descripciones de producto
        description_selectors = [
            '[class*="product-description"]',
            '[class*="description"]',
            '[class*="summary"]',
            '[class*="overview"]',
            '[class*="details"]',
            '[data-testid*="description"]',
            '[class*="product-info"]',
            '[class*="caracteristicas"]',
            'meta[name="description"]'
        ]
        
        # Elementos a excluir
        excluded_classes = [
            'nav', 'menu', 'header', 'footer', 'sidebar', 'cart', 'carrito',
            'checkout', 'payment', 'shipping', 'delivery', 'price', 'precio',
            'review', 'opinion', 'rating', 'valoracion', 'breadcrumb'
        ]
        
        for selector in description_selectors:
            if 'meta' in selector:
                element = soup.select_one(selector)
                if element:
                    desc = element.get('content', '')
                    if desc and len(desc) > 30:
                        description += desc + " "
            else:
                elements = soup.select(selector)
                for element in elements:
                    # Verificar que no sea un elemento excluido
                    element_class = element.get('class', [])
                    element_id = element.get('id', '')
                    
                    is_excluded = any(
                        excluded in str(element_class).lower() or 
                        excluded in element_id.lower() 
                        for excluded in excluded_classes
                    )
                    
                    if not is_excluded:
                        text = element.get_text().strip()
                        if text and len(text) > 30 and len(text) < 3000:
                            if not self._is_ecommerce_text(text):
                                description += text + " "
        
        return description.strip()
    
    def _is_ecommerce_text(self, text):
        """Detecta si un texto es relacionado con e-commerce y no con producto"""
        text_lower = text.lower()
        
        # Patrones que indican texto de e-commerce
        ecommerce_patterns = [
            'a√±adir al carrito', 'comprar ahora', 'env√≠o gratis',
            'opiniones de', 'valoraciones de', 'pol√≠tica de',
            'mi cuenta', 'iniciar sesi√≥n', 'comparar producto',
            'stock disponible', 'descuento del', 'gastos de env√≠o'
        ]
        
        pattern_count = sum(1 for pattern in ecommerce_patterns if pattern in text_lower)
        
        # Si m√°s del 30% del texto son palabras de e-commerce, lo descartamos
        words = text_lower.split()
        ecommerce_word_count = sum(1 for word in words if word in self.stop_words)
        ecommerce_ratio = ecommerce_word_count / len(words) if words else 0
        
        return pattern_count > 2 or ecommerce_ratio > 0.3
    
    def _extract_features(self, soup):
        """Extrae caracter√≠sticas y features del producto"""
        features = []
        
        # Buscar listas de caracter√≠sticas
        feature_selectors = [
            '[class*="feature"] li',
            '[class*="benefit"] li',
            '[class*="highlight"] li',
            '[class*="spec"] li',
            'ul[class*="feature"] li',
            '.features li',
            '.benefits li',
            'div[class*="feature"]'
        ]
        
        for selector in feature_selectors:
            elements = soup.select(selector)
            for element in elements:
                text = element.get_text().strip()
                if (text and 
                    len(text) > 10 and 
                    len(text) < 500 and 
                    not re.match(r'^\d+$', text) and
                    not text.lower().startswith(('http', 'www', 'mailto'))):
                    features.append(text)
        
        # Eliminar duplicados manteniendo orden
        seen = set()
        unique_features = []
        for feature in features:
            if feature.lower() not in seen:
                seen.add(feature.lower())
                unique_features.append(feature)
        
        return unique_features[:50]
    
    def _extract_specifications(self, soup):
        """Extrae especificaciones t√©cnicas"""
        specs = {}
        
        # Buscar tablas de especificaciones
        spec_selectors = [
            'table[class*="spec"]',
            'table[class*="detail"]',
            'table[class*="tech"]',
            'dl[class*="spec"]',
            'table'
        ]
        
        for selector in spec_selectors:
            elements = soup.select(selector)
            for element in elements:
                if element.name == 'table':
                    rows = element.find_all('tr')
                    for row in rows:
                        cells = row.find_all(['td', 'th'])
                        if len(cells) >= 2:
                            key = cells[0].get_text().strip()
                            value = cells[1].get_text().strip()
                            if key and value and len(key) < 100 and len(value) < 200:
                                specs[key] = value
                elif element.name == 'dl':
                    dts = element.find_all('dt')
                    dds = element.find_all('dd')
                    for dt, dd in zip(dts, dds):
                        key = dt.get_text().strip()
                        value = dd.get_text().strip()
                        if key and value:
                            specs[key] = value
        
        return specs
    
    def _extract_price(self, soup):
        """Extrae informaci√≥n de precio"""
        price_selectors = [
            '[class*="price"]',
            '[class*="cost"]',
            '[class*="amount"]',
            '[data-testid*="price"]',
            '[id*="price"]',
            'span[itemprop="price"]',
            'meta[itemprop="price"]'
        ]
        
        for selector in price_selectors:
            elements = soup.select(selector)
            for element in elements:
                if element.name == 'meta':
                    price = element.get('content', '')
                    if price:
                        return price
                else:
                    text = element.get_text().strip()
                    # Buscar patrones de precio
                    price_patterns = [
                        r'[‚Ç¨$¬£¬•]\s*[\d,]+\.?\d*',
                        r'[\d,]+\.?\d*\s*[‚Ç¨$¬£¬•]',
                        r'[\d,]+\.?\d*\s*EUR?'
                    ]
                    
                    for pattern in price_patterns:
                        price_match = re.search(pattern, text, re.IGNORECASE)
                        if price_match:
                            return price_match.group().strip()
        
        return ""
    
    def _extract_filters(self, soup):
        """Extrae filtros disponibles en la p√°gina"""
        filters = []
        
        filter_selectors = [
            '[class*="filter"] a',
            '[class*="facet"] a',
            'select option',
            '[type="checkbox"] + label',
            '[class*="refinement"]'
        ]
        
        for selector in filter_selectors:
            elements = soup.select(selector)
            for element in elements:
                text = element.get_text().strip()
                if (text and 
                    len(text) > 2 and 
                    len(text) < 80 and
                    not text.lower().startswith(('http', 'www')) and
                    not re.match(r'^\d+$', text)):
                    filters.append(text)
        
        return list(set(filters))[:100]
    
    def _extract_categories(self, soup):
        """Extrae categor√≠as del producto"""
        categories = []
        
        category_selectors = [
            '[class*="breadcrumb"] a',
            '[class*="category"] a',
            '.breadcrumb a',
            'nav[aria-label*="breadcrumb"] a'
        ]
        
        for selector in category_selectors:
            elements = soup.select(selector)
            for element in elements:
                text = element.get_text().strip()
                if (text and 
                    text.lower() not in ['home', 'inicio', 'tienda'] and
                    len(text) > 2 and 
                    len(text) < 50):
                    categories.append(text)
        
        return categories
    
    def _extract_images(self, soup):
        """Extrae URLs de im√°genes del producto"""
        images = []
        
        image_selectors = [
            'img[class*="product"]',
            'img[data-testid*="product"]',
            '[class*="gallery"] img',
            '[class*="image"] img',
            'picture img'
        ]
        
        for selector in image_selectors:
            elements = soup.select(selector)
            for element in elements:
                src = element.get('src') or element.get('data-src')
                if src and not any(x in src.lower() for x in ['placeholder', 'loading', 'spinner']):
                    images.append(src)
        
        return list(set(images))[:10]
    
    def analyze_terms(self, all_data):
        """Analiza los t√©rminos m√°s frecuentes enfoc√°ndose en caracter√≠sticas de producto"""
        all_text = ""
        
        for data in all_data:
            # Priorizar t√≠tulo y caracter√≠sticas
            title_text = data.get('title', '')
            features_text = " ".join(data.get('features', []))
            specs_keys = " ".join(data.get('specifications', {}).keys())
            specs_values = " ".join(data.get('specifications', {}).values())
            
            # Dar m√°s peso a caracter√≠sticas t√©cnicas
            all_text += f" {title_text} {features_text} {features_text} {specs_keys} {specs_values} "
            
            # Agregar descripci√≥n filtrada
            description = data.get('description', '')
            if description:
                sentences = description.split('.')
                for sentence in sentences:
                    if self._is_product_relevant_sentence(sentence):
                        all_text += sentence + " "
        
        # Limpiar y tokenizar texto
        words = re.findall(r'\b[a-z√°√©√≠√≥√∫√±√ºA-Z√Å√â√ç√ì√ö√ë√ú]{3,}\b', all_text.lower())
        
        # Filtrar palabras relevantes
        filtered_words = []
        for word in words:
            if (word not in self.stop_words and 
                len(word) >= 3 and 
                not word.isdigit() and
                self._is_product_term(word)):
                filtered_words.append(word)
        
        return Counter(filtered_words)
    
    def _is_product_relevant_sentence(self, sentence):
        """Determina si una oraci√≥n es relevante para el producto"""
        sentence_lower = sentence.lower().strip()
        
        # Frases que indican caracter√≠sticas t√©cnicas
        positive_indicators = [
            'caracter√≠sticas', 'especificaciones', 'incluye', 'cuenta con',
            'tecnolog√≠a', 'material', 'dise√±o', 'tama√±o', 'dimensiones',
            'memoria', 'procesador', 'pantalla', 'bater√≠a', 'compatible'
        ]
        
        # Frases no relevantes
        negative_indicators = [
            'a√±adir', 'carrito', 'comprar', 'precio', 'env√≠o',
            'opini√≥n', 'valoraci√≥n', 'stock', 'oferta', 'cliente'
        ]
        
        positive_score = sum(1 for indicator in positive_indicators if indicator in sentence_lower)
        negative_score = sum(1 for indicator in negative_indicators if indicator in sentence_lower)
        
        return positive_score > negative_score and len(sentence.strip()) > 20
    
    def _is_product_term(self, word):
        """Determina si una palabra es relevante para describir productos"""
        irrelevant_terms = {
            'p√°gina', 'sitio', 'web', 'usuario', 'cliente', 'cuenta',
            'compra', 'pedido', 'pago', 'env√≠o', 'precio', 'oferta',
            'opini√≥n', 'valoraci√≥n', 'comentario', 'estrella'
        }
        
        return word not in irrelevant_terms
    
    def analyze_filters(self, all_data):
        """Analiza los filtros m√°s comunes"""
        all_filters = []
        
        for data in all_data:
            all_filters.extend(data.get('filters', []))
        
        return Counter(all_filters)
    
    def analyze_features(self, all_data):
        """Analiza las caracter√≠sticas m√°s mencionadas"""
        all_features = []
        
        for data in all_data:
            all_features.extend(data.get('features', []))
        
        # Extraer palabras clave de las caracter√≠sticas
        feature_words = []
        for feature in all_features:
            words = re.findall(r'\b[a-z√°√©√≠√≥√∫√±√ºA-Z√Å√â√ç√ì√ö√ë√ú]{3,}\b', feature.lower())
            words = [word for word in words if word not in self.stop_words]
            feature_words.extend(words)
        
        return Counter(feature_words)
    
    def analyze_gaps(self, reference_data, comparison_data):
        """Analiza gaps entre producto de referencia y competencia"""
        gaps = {
            'missing_features': [],
            'missing_specs': [],
            'missing_filters': [],
            'unique_competitor_features': [],
            'price_difference': None,
            'category_differences': []
        }
        
        if not reference_data or not comparison_data:
            return gaps
        
        # Analizar caracter√≠sticas faltantes
        ref_features = set([f.lower() for f in reference_data.get('features', [])])
        
        for comp_data in comparison_data:
            comp_features = set([f.lower() for f in comp_data.get('features', [])])
            
            # Caracter√≠sticas que tiene la competencia pero no la referencia
            unique_comp = comp_features - ref_features
            gaps['unique_competitor_features'].extend(list(unique_comp))
            
            # Caracter√≠sticas que tiene la referencia pero no la competencia
            missing = ref_features - comp_features
            gaps['missing_features'].extend(list(missing))
        
        # Analizar especificaciones faltantes
        ref_specs = set(reference_data.get('specifications', {}).keys())
        
        for comp_data in comparison_data:
            comp_specs = set(comp_data.get('specifications', {}).keys())
            missing_specs = ref_specs - comp_specs
            gaps['missing_specs'].extend(list(missing_specs))
        
        # Analizar filtros
        ref_filters = set(reference_data.get('filters', []))
        all_comp_filters = set()
        
        for comp_data in comparison_data:
            all_comp_filters.update(comp_data.get('filters', []))
        
        gaps['missing_filters'] = list(all_comp_filters - ref_filters)
        
        # Analizar diferencias de precio
        ref_price = self._extract_price_value(reference_data.get('price', ''))
        if ref_price:
            comp_prices = []
            for comp_data in comparison_data:
                comp_price = self._extract_price_value(comp_data.get('price', ''))
                if comp_price:
                    comp_prices.append(comp_price)
            
            if comp_prices:
                avg_comp_price = sum(comp_prices) / len(comp_prices)
                gaps['price_difference'] = {
                    'reference': ref_price,
                    'competitors_avg': avg_comp_price,
                    'difference': ref_price - avg_comp_price,
                    'percentage': ((ref_price - avg_comp_price) / avg_comp_price) * 100
                }
        
        # Eliminar duplicados
        gaps['missing_features'] = list(set(gaps['missing_features']))
        gaps['missing_specs'] = list(set(gaps['missing_specs']))
        gaps['unique_competitor_features'] = list(set(gaps['unique_competitor_features']))
        
        return gaps
    
    def _extract_price_value(self, price_text):
        """Extrae el valor num√©rico del precio"""
        if not price_text:
            return None
        
        # Buscar n√∫meros en el texto del precio
        price_match = re.search(r'[\d,]+\.?\d*', price_text.replace(',', ''))
        if price_match:
            try:
                return float(price_match.group())
            except:
                return None
        return None


if __name__ == "__main__":
    main()
